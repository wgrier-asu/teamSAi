{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== classic_control =====\n",
      "Acrobot-v1             CartPole-v0            CartPole-v1\n",
      "MountainCar-v0         MountainCarContinuous-v0 Pendulum-v1\n",
      "===== phys2d =====\n",
      "phys2d/CartPole-v0     phys2d/CartPole-v1     phys2d/Pendulum-v0\n",
      "===== box2d =====\n",
      "BipedalWalker-v3       BipedalWalkerHardcore-v3 CarRacing-v2\n",
      "LunarLander-v2         LunarLanderContinuous-v2\n",
      "===== toy_text =====\n",
      "Blackjack-v1           CliffWalking-v0        FrozenLake-v1\n",
      "FrozenLake8x8-v1       Taxi-v3\n",
      "===== tabular =====\n",
      "tabular/Blackjack-v0   tabular/CliffWalking-v0\n",
      "===== mujoco =====\n",
      "Ant-v2                 Ant-v3                 Ant-v4\n",
      "HalfCheetah-v2         HalfCheetah-v3         HalfCheetah-v4\n",
      "Hopper-v2              Hopper-v3              Hopper-v4\n",
      "Humanoid-v2            Humanoid-v3            Humanoid-v4\n",
      "HumanoidStandup-v2     HumanoidStandup-v4     InvertedDoublePendulum-v2\n",
      "InvertedDoublePendulum-v4 InvertedPendulum-v2    InvertedPendulum-v4\n",
      "Pusher-v2              Pusher-v4              Reacher-v2\n",
      "Reacher-v4             Swimmer-v2             Swimmer-v3\n",
      "Swimmer-v4             Walker2d-v2            Walker2d-v3\n",
      "Walker2d-v4\n",
      "===== None =====\n",
      "GymV21Environment-v0   GymV26Environment-v0\n",
      "===== envs =====\n",
      "Boxoban-Test-v0        Boxoban-Train-v0       Boxoban-Train-v1\n",
      "Boxoban-Val-v0         Boxoban-Val-v1         FixedBoxoban-Test-v0\n",
      "FixedBoxoban-Train-v0  FixedBoxoban-Train-v1  FixedBoxoban-Val-v0\n",
      "FixedBoxoban-Val-v1    FixedTarget-Sokoban-v0 FixedTarget-Sokoban-v1\n",
      "FixedTarget-Sokoban-v2 FixedTarget-Sokoban-v3 PushAndPull-Sokoban-v0\n",
      "PushAndPull-Sokoban-v1 PushAndPull-Sokoban-v2 PushAndPull-Sokoban-v3\n",
      "PushAndPull-Sokoban-v4 PushAndPull-Sokoban-v5 SideEffects-v0\n",
      "Sokoban-huge-v0        Sokoban-large-v0       Sokoban-large-v1\n",
      "Sokoban-large-v2       Sokoban-small-v0       Sokoban-small-v1\n",
      "Sokoban-v0             Sokoban-v1             Sokoban-v2\n",
      "TwoPlayer-Sokoban-v0   TwoPlayer-Sokoban-v1   TwoPlayer-Sokoban-v2\n",
      "TwoPlayer-Sokoban-v3   TwoPlayer-Sokoban-v4   TwoPlayer-Sokoban-v5\n",
      "\n",
      "\n",
      "Making environment...\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import HumanRendering, OrderEnforcing, RecordEpisodeStatistics\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "\n",
    "# https://github.com/AlignmentResearch/gym-sokoban/tree/default\n",
    "# Download gym-sokoban and build library locally\n",
    "import gym_sokoban\n",
    "\n",
    "# You should see the sokoban environments in this list:\n",
    "gym.pprint_registry()\n",
    "\n",
    "# Create the Sokoban environment\n",
    "env_name = 'Sokoban-v2'\n",
    "SEED = 1\n",
    "max_steps = 20\n",
    "\n",
    "print('\\n\\nMaking environment...')\n",
    "env = gym.make(id=env_name,\n",
    "               max_episode_steps=max_steps,\n",
    "               max_steps=max_steps,\n",
    "               tinyworld_obs=True,\n",
    "               tinyworld_render=False,\n",
    "               reset=True,\n",
    "               terminate_on_first_box=False,\n",
    "               reset_seed=SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Q-Learning agent\n",
    "class QLearningAgent:\n",
    "    def __init__(self, env, learning_rate=0.1, discount_factor=0.9, exploration_prob=0.1, num_tiles=4):\n",
    "        self.env = env\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "        self.exploration_prob = exploration_prob\n",
    "        self.num_tiles = num_tiles\n",
    "        self.action_space_n = env.action_space.n\n",
    "\n",
    "        self.tile_coder = TileCoder(env.observation_space.low[0], env.observation_space.high[0], num_tiles, env.action_space.n)\n",
    "        self.q_table = np.zeros((self.tile_coder.total_tiles, self.action_space_n))\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        if random.uniform(0, 1) < self.exploration_prob:\n",
    "            return random.randint(0, self.action_space_n - 1)  # Explore\n",
    "        else:\n",
    "            return np.argmax(self.q_table[state])  # Exploit\n",
    "\n",
    "    def update_q_table(self, state, action, reward, next_state):\n",
    "        best_next_action = np.argmax(self.q_table[next_state])\n",
    "        self.q_table[state, action] += self.learning_rate * (reward + self.discount_factor * self.q_table[next_state, best_next_action] - self.q_table[state, action])\n",
    "\n",
    "    def train(self, num_episodes):\n",
    "        for episode in range(num_episodes):\n",
    "            state = self.tile_coder.encode(self.env.reset())\n",
    "            total_reward = 0\n",
    "            done = False\n",
    "            while not done:\n",
    "                action = self.choose_action(state)\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                next_state = self.tile_coder.encode(next_state)\n",
    "                self.update_q_table(state, action, reward, next_state)\n",
    "                state = next_state\n",
    "                total_reward += reward\n",
    "            print(f\"Episode {episode + 1}: Total Reward = {total_reward}\")\n",
    "\n",
    "class TileCoder:\n",
    "    def __init__(self, low, high, num_tiles, num_actions):\n",
    "        self.low = low\n",
    "        self.high = high\n",
    "        self.num_tiles = num_tiles\n",
    "        self.num_actions = num_actions\n",
    "        self.dimensions = len(low)\n",
    "        self.total_tiles = (num_tiles ** self.dimensions) * num_actions\n",
    "        self.tile_widths = (high - low) / num_tiles\n",
    "\n",
    "    def encode(self, state):\n",
    "        #since the sokoban enviroment adds a dictonary for some reason\n",
    "        state = state[0]\n",
    "        indices = []\n",
    "        print(self.dimensions)\n",
    "        for i in range(self.dimensions):\n",
    "            print(i)\n",
    "            print(state[0][i])\n",
    "            vector = np.vectorize(np.int_)\n",
    "            index = int((state[0][i] - self.low[i]) / self.tile_widths[i])\n",
    "            print(index)\n",
    "            indices.append(index)\n",
    "        return sum([index * (self.num_tiles ** i) for i, index in enumerate(indices)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "making Agent\n"
     ]
    }
   ],
   "source": [
    "print(\"making Agent\")\n",
    "qAgent = QLearningAgent(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainning Agent\n",
      "10\n",
      "0\n",
      "[0 0 0]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "only length-1 arrays can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrainning Agent\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m numEpisodes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mqAgent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnumEpisodes\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 26\u001b[0m, in \u001b[0;36mQLearningAgent.train\u001b[0;34m(self, num_episodes)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(\u001b[38;5;28mself\u001b[39m, num_episodes):\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m episode \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_episodes):\n\u001b[0;32m---> 26\u001b[0m         state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtile_coder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m         total_reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     28\u001b[0m         done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[5], line 57\u001b[0m, in \u001b[0;36mTileCoder.encode\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28mprint\u001b[39m(state[\u001b[38;5;241m0\u001b[39m][i])\n\u001b[1;32m     56\u001b[0m vector \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mvectorize(np\u001b[38;5;241m.\u001b[39mint_)\n\u001b[0;32m---> 57\u001b[0m index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlow\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtile_widths\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28mprint\u001b[39m(index)\n\u001b[1;32m     59\u001b[0m indices\u001b[38;5;241m.\u001b[39mappend(index)\n",
      "\u001b[0;31mTypeError\u001b[0m: only length-1 arrays can be converted to Python scalars"
     ]
    }
   ],
   "source": [
    "print(\"trainning Agent\")\n",
    "numEpisodes=10\n",
    "qAgent.train(numEpisodes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#eval runs\n",
    "\n",
    "ACTION_LOOKUP = env.unwrapped.get_action_lookup()\n",
    "\n",
    "for i_episode in range(episodes):#20\n",
    "    print('\\n\\nStarting episode #{}'.format(i_episode+1))\n",
    "    observation, info = env.reset()\n",
    "    \n",
    "\n",
    "    for t in range(max_steps+10):#100\n",
    "        env.render()\n",
    "\n",
    "\n",
    "        # action = env.action_space.sample()\n",
    "        #action = int(input(\"Enter action ==> \"))\n",
    "        action = QLearningAgent()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Sleep makes the actions visible for users\n",
    "        time.sleep(1)\n",
    "        observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "\n",
    "\n",
    "        print(\"a=[{}] r={} done={}||{} info={}\".format(ACTION_LOOKUP[action], reward, terminated, truncated, info))\n",
    "        if terminated or truncated:\n",
    "            print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "            if(truncated): print(\"Reason: Truncated\")\n",
    "            else: print(\"Reason: Terminated\")\n",
    "            env.render()\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "safetyClass",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
